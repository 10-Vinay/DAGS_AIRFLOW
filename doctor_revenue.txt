from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, date_format, sha2, concat_ws, col, to_timestamp,substring
from sqlalchemy import text
from google.cloud.sql.connector import Connector, IPTypes
import os
from configparser import ConfigParser
import pg8000
import sqlalchemy

# Define the Python function to process the CSV file with PySpark
def process_csv(**kwargs):
    spark = SparkSession.builder \
        .appName("Process CSV Example") \
        .getOrCreate()

    
    csv_file_path = "/opt/airflow/data/Doctor_Rev.csv"  

    df = spark.read.csv(csv_file_path, header=True, inferSchema=True)

    df = df.withColumn("Date", to_date(df["Date"], "M/d/yyyy"))
    
    df = df.withColumn("Time", to_timestamp(df["Time"], "HH:mm:ss"))
    df = df.withColumn("Time", substring(df["Time"].cast("string"), 12, 8))

    
    concat_cols = concat_ws("", *[df[col].cast("string") for col in df.columns])

    df = df.withColumn("hash_id", sha2(concat_cols, 256))

    # Convert DataFrame to JSON and push to XCom
    preprocessed_data_json = df.toJSON().collect()
    kwargs['ti'].xcom_push(key='preprocessed_data', value=preprocessed_data_json)

    # Stop the Spark session
    spark.stop()

def get_database_connection():
    config = ConfigParser()
    config.read('/opt/airflow/config.ini')  # Assuming config file is in the 'app' directory
    db_config = config['database']

    db_user = 'postgres'  # e.g. 'my-database-user'
    db_pass = 'a55zWlt:CYtAi|FB(|jJSpRA90N}'  # e.g. 'my-database-password'
    db_name = 'data-lake-table' #'fingpt-db-dev'  # e.g. 'my-database'
    unix_socket_path = 'independent-way-410316:us-central1:collaborativedocsdb-test' 
    ip_type = IPTypes.PRIVATE if os.environ.get("PRIVATE_IP") else IPTypes.PUBLIC

    # initialize Cloud SQL Python Connector object
    connector = Connector()

    def getconn() -> pg8000.dbapi.Connection:
        conn: pg8000.dbapi.Connection = connector.connect(
            unix_socket_path,
            "pg8000",
            user=db_user,
            password=db_pass,
            db=db_name,
            ip_type=ip_type
        )
        return conn

    pool = sqlalchemy.create_engine(
        "postgresql+pg8000://",
        creator=getconn,
        pool_size=5,
        max_overflow=2,
        pool_timeout=30,  # 30 seconds
        pool_recycle=1800  # 30 minutes
    )
    return pool

def create_doctor_rev_table(engine):
    create_table_query = """
     CREATE TABLE IF NOT EXISTS py_doctor_revenue (
        "Group" VARCHAR,
        "Bill_Type" VARCHAR,
        "Bill_No" VARCHAR,
        "Patient_Name" VARCHAR,
        "City" VARCHAR,
        "Doctor_Name" VARCHAR,
        "Billing_Code" VARCHAR,
        "Service_Type" VARCHAR,
        "Qty" DECIMAL,
        "Billed" DECIMAL,
        "Disc_Amt" DECIMAL,
        "Net_Amt" DECIMAL,
        "Date" DATE,
        "Time" TIME,
        "Month_Year" VARCHAR,
        "hash_id" VARCHAR PRIMARY KEY
    )
    """
    with engine.connect() as conn:
        conn.execute(create_table_query)

def bulk_insert_data_to_postgres(engine, rows):
    records = []
    for row in rows:
        records.append({
            'Group': row['Group'],
            'Bill_Type': row['Bill Type'],
            'Bill_No': row['Bill No'],
            'Patient_Name': row['Patient Name'],
            'City': row['City'],
            'Doctor_Name': row['Doctor Name'],
            'Billing_Code': row['Billing Code'],
            'Service_Type': row['Service Type'],
            'Qty': row['Qty'],
            'Billed': row['Billed'],
            'Disc_Amt': row['Disc Amt'],
            'Net_Amt': row['Net Amt'],
            'Date': row['Date'],
            'Time': row['Time'],
            'Month_Year': row['Month-Year'],
            'hash_id': row['hash_id']
        })

    insert_query = text("""
        INSERT INTO py_doctor_revenue (
            "Group", "Bill_Type", "Bill_No", "Patient_Name", "City", "Doctor_Name", "Billing_Code", "Service_Type", "Qty", "Billed", "Disc_Amt", "Net_Amt", "Date", "Time", "Month_Year", "hash_id"
        ) VALUES (
            :Group, :Bill_Type, :Bill_No, :Patient_Name, :City, :Doctor_Name, :Billing_Code, :Service_Type, :Qty, :Billed, :Disc_Amt, :Net_Amt, :Date, :Time, :Month_Year, :hash_id
        ) ON CONFLICT (hash_id) DO NOTHING;
    """)

    batch_size = 1000
    with engine.connect() as conn:
        for i in range(0, len(records), batch_size):
            batch_records = records[i:i+batch_size]
            print(f"Inserting Rows {i} to {i+batch_size}")
            conn.execute(insert_query, batch_records)

def insert_data_to_postgres(**kwargs):
    ti = kwargs['ti']
    preprocessed_data_json = ti.xcom_pull(key='preprocessed_data', task_ids='process_csv')
    
    # Convert JSON string back to DataFrame
    spark = SparkSession.builder.appName("Doctor_Rev_csv_to_postgres").getOrCreate()
    df = spark.read.json(spark.sparkContext.parallelize(preprocessed_data_json))
    
    # Collect data as a list of Row objects
    rows = df.collect()
    
    engine = get_database_connection()
    
    # Create the table if it doesn't exist
    create_doctor_rev_table(engine)
    
    # Insert data into PostgreSQL
    bulk_insert_data_to_postgres(engine, rows)

# Define the default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 7, 1),
    'retries': 1,
}

with DAG(
    'doc_2',
    default_args=default_args,
    description='A DAG to process doctor revenue CSV file and push to PostgreSQL',
    schedule_interval='@daily',
    catchup=False,
) as dag:

    process_csv_task = PythonOperator(
        task_id='process_csv',
        python_callable=process_csv,
        provide_context=True,
    )

    # Define the task to insert data into PostgreSQL
    insert_data_to_postgres_task = PythonOperator(
        task_id='insert_data_to_postgres',
        python_callable=insert_data_to_postgres,
        provide_context=True,
    )

    # Set the task dependencies
    process_csv_task >> insert_data_to_postgres_task
